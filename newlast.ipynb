{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #加载一些统计的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_table('train.txt',sep=' ',index_col=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('1234.csv',index_col=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.pop('item_category_list')\n",
    "data.insert(1, 'item_category_list', data2[['item_category_list']])\n",
    "data.pop('item_property_list')\n",
    "data.insert(2, 'item_property_list', data2[['item_property_list']])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[['context_timestamp']] = data[['context_timestamp']] - 1537200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[['item_category_list','item_property_list',\n",
    "          'shop_review_positive_rate','shop_score_service',\n",
    "         'shop_score_delivery','shop_score_description','shop_star_level'\n",
    "          ,'shop_review_num_level','item_price_level','item_sales_level',\n",
    "         'item_collected_level','item_pv_level',\n",
    "          'user_gender_id','user_age_level',\n",
    "          'user_occupation_id','user_star_level',\n",
    "          'context_page_id','context_timestamp']]\n",
    "Y = data[['is_trade']]\n",
    "X = np.array(X)\n",
    "Y = np.array(Y).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 制作训练集与测试集并且进行预处理\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_shop_train = X_train[:, :6]\n",
    "X_item_train = X_train[:, 6:10]\n",
    "X_user_train = X_train[:, 10:13]\n",
    "X_content_train = X_train[:, 13:15]\n",
    "X_shop_test = X_test[:, :6]\n",
    "X_item_test = X_test[:, 6:10]\n",
    "X_user_test = X_test[:, 10:13]\n",
    "X_content_test = X_test[:, 13:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = X_train\n",
    "test = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = pd.read_table('test.txt',sep=' ',index_col=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data3 = pd.read_csv('456.csv',index_col=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2.pop('item_category_list')\n",
    "data2.insert(1, 'item_category_list', data3[['item_category_list']])\n",
    "data2.pop('item_property_list')\n",
    "data2.insert(2, 'item_property_list', data3[['item_property_list']])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2[['context_timestamp']] = data[['context_timestamp']] - 1537200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pre = data2[['item_category_list','item_property_list',\n",
    "          'shop_review_positive_rate','shop_score_service',\n",
    "         'shop_score_delivery','shop_score_description','shop_star_level'\n",
    "          ,'shop_review_num_level','item_price_level','item_sales_level',\n",
    "         'item_collected_level','item_pv_level',\n",
    "          'user_gender_id','user_age_level',\n",
    "          'user_occupation_id','user_star_level',\n",
    "          'context_page_id','context_timestamp']]\n",
    "X_pre = np.array(X_pre,dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分箱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17\n",
      "[-1.5   0.33  1.33  2.33]\n",
      "0k\n",
      "2.21333333333\n",
      "[-1.5         0.28666667  1.28666667  2.28666667]\n",
      "0k\n",
      "23.0\n",
      "[ -1.50000000e+00   4.99850000e+03   4.99950000e+03   5.00050000e+03\n",
      "   5.00150000e+03   5.00250000e+03   5.00350000e+03   5.00450000e+03\n",
      "   5.00550000e+03   5.00650000e+03   5.00750000e+03   5.00850000e+03\n",
      "   5.00950000e+03   5.01050000e+03   5.01150000e+03   5.01250000e+03\n",
      "   5.01350000e+03   5.01450000e+03   5.01550000e+03   5.01650000e+03\n",
      "   5.01750000e+03   5.01850000e+03   5.01950000e+03   5.02050000e+03]\n",
      "0k\n",
      "27.0\n",
      "[ -1.5  -0.5   0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5\n",
      "  10.5  11.5  12.5  13.5  14.5  15.5  16.5  17.5  18.5  19.5  20.5  21.5\n",
      "  22.5  23.5  24.5  25.5]\n",
      "0k\n",
      "19.0\n",
      "[ -1.5  -0.5   0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5\n",
      "  10.5  11.5  12.5  13.5  14.5  15.5  16.5  17.5]\n",
      "0k\n",
      "18.0\n",
      "[ -1.5   0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5  10.5\n",
      "  11.5  12.5  13.5  14.5  15.5  16.5  17.5]\n",
      "0k\n",
      "19.0\n",
      "[ -1.5  -0.5   0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5\n",
      "  10.5  11.5  12.5  13.5  14.5  15.5  16.5  17.5]\n",
      "0k\n",
      "23.0\n",
      "[ -1.5  -0.5   0.5   1.5   2.5   3.5   4.5   5.5   6.5   7.5   8.5   9.5\n",
      "  10.5  11.5  12.5  13.5  14.5  15.5  16.5  17.5  18.5  19.5  20.5  21.5]\n",
      "0k\n",
      "4.0\n",
      "[-1.5 -0.5  0.5  1.5  2.5]\n",
      "0k\n",
      "9.0\n",
      "[   -1.5   999.5  1000.5  1001.5  1002.5  1003.5  1004.5  1005.5  1006.5\n",
      "  1007.5]\n",
      "0k\n",
      "5.0\n",
      "[ -1.50000000e+00   2.00150000e+03   2.00250000e+03   2.00350000e+03\n",
      "   2.00450000e+03   2.00550000e+03]\n",
      "0k\n",
      "12.0\n",
      "[ -1.50000000e+00   2.99950000e+03   3.00050000e+03   3.00150000e+03\n",
      "   3.00250000e+03   3.00350000e+03   3.00450000e+03   3.00550000e+03\n",
      "   3.00650000e+03   3.00750000e+03   3.00850000e+03   3.00950000e+03\n",
      "   3.01050000e+03]\n",
      "0k\n",
      "21.0\n",
      "[ -1.50000000e+00   4.00050000e+03   4.00150000e+03   4.00250000e+03\n",
      "   4.00350000e+03   4.00450000e+03   4.00550000e+03   4.00650000e+03\n",
      "   4.00750000e+03   4.00850000e+03   4.00950000e+03   4.01050000e+03\n",
      "   4.01150000e+03   4.01250000e+03   4.01350000e+03   4.01450000e+03\n",
      "   4.01550000e+03   4.01650000e+03   4.01750000e+03   4.01850000e+03\n",
      "   4.01950000e+03   4.02050000e+03]\n",
      "0k\n"
     ]
    }
   ],
   "source": [
    "#item_price_level\n",
    "# bins = np.linspace(-0.5, 2.5, 4)\n",
    "sha = train.shape[1]\n",
    "for i in range(4,sha-1):\n",
    "# i = 4\n",
    "# if True:\n",
    "    print((X_train[:,i][X_train[:,i]>=0].max()+1.5)-(X_train[:,i][X_train[:,i]>=0].min()-0.5))\n",
    "    bins = np.arange(X_train[:,i][(X_train[:,i]>=0)].min()-0.5, X_train[:,i][X_train[:,i]>=0].max()+1.5, 1)\n",
    "    bins = np.hstack(([-1.5],bins))\n",
    "    which_bin = np.digitize(train[:,i].reshape(-1, 1), bins=bins)\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoder.fit(which_bin)\n",
    "    X_train_binned = encoder.transform(which_bin)\n",
    "\n",
    "\n",
    "\n",
    "    which_bin = np.digitize(test[:,i].reshape(-1, 1), bins=bins)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     encoder.fit(which_bin)\n",
    "    X_test_binned = encoder.transform(which_bin)\n",
    "    \n",
    "    which_bin = np.digitize(X_pre[:,i].reshape(-1, 1), bins=bins)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     encoder.fit(which_bin)\n",
    "    X_pre_binned = encoder.transform(which_bin)\n",
    "    print(bins)\n",
    "    if (np.hstack((train,X_train_binned)).shape[1] == np.hstack((test,X_test_binned)).shape[1]):\n",
    "        if (np.hstack((X_pre,X_pre_binned)).shape[1] == np.hstack((test,X_test_binned)).shape[1]):\n",
    "            train = np.hstack((train,X_train_binned))\n",
    "            test = np.hstack((test,X_test_binned))\n",
    "            X_pre = np.hstack((X_pre,X_pre_binned))\n",
    "            print(\"0k\")\n",
    "    else:\n",
    "        print(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0.   1800.   3600.   5400.   7200.   9000.  10800.  12600.  14400.\n",
      "  16200.  18000.  19800.  21600.  23400.  25200.  27000.  28800.  30600.\n",
      "  32400.  34200.  36000.  37800.  39600.  41400.  43200.  45000.  46800.\n",
      "  48600.  50400.  52200.  54000.  55800.  57600.  59400.  61200.  63000.\n",
      "  64800.  66600.  68400.  70200.  72000.  73800.  75600.  77400.  79200.\n",
      "  81000.  82800.  84600.  86400.]\n",
      "[ -1.50000000e+00   0.00000000e+00   1.80000000e+03   3.60000000e+03\n",
      "   5.40000000e+03   7.20000000e+03   9.00000000e+03   1.08000000e+04\n",
      "   1.26000000e+04   1.44000000e+04   1.62000000e+04   1.80000000e+04\n",
      "   1.98000000e+04   2.16000000e+04   2.34000000e+04   2.52000000e+04\n",
      "   2.70000000e+04   2.88000000e+04   3.06000000e+04   3.24000000e+04\n",
      "   3.42000000e+04   3.60000000e+04   3.78000000e+04   3.96000000e+04\n",
      "   4.14000000e+04   4.32000000e+04   4.50000000e+04   4.68000000e+04\n",
      "   4.86000000e+04   5.04000000e+04   5.22000000e+04   5.40000000e+04\n",
      "   5.58000000e+04   5.76000000e+04   5.94000000e+04   6.12000000e+04\n",
      "   6.30000000e+04   6.48000000e+04   6.66000000e+04   6.84000000e+04\n",
      "   7.02000000e+04   7.20000000e+04   7.38000000e+04   7.56000000e+04\n",
      "   7.74000000e+04   7.92000000e+04   8.10000000e+04   8.28000000e+04\n",
      "   8.46000000e+04   8.64000000e+04]\n",
      "0k\n"
     ]
    }
   ],
   "source": [
    "bins = np.linspace(0, 86400,49)\n",
    "print(bins)\n",
    "bins = np.hstack(([-1.5],bins))\n",
    "which_bin = np.digitize(train[:,-1].reshape(-1, 1), bins=bins)\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(which_bin)\n",
    "X_train_binned = encoder.transform(which_bin)\n",
    "\n",
    "\n",
    "\n",
    "which_bin = np.digitize(test[:,-1].reshape(-1, 1), bins=bins)\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# encoder.fit(which_bin)\n",
    "X_test_binned = encoder.transform(which_bin)\n",
    "\n",
    "which_bin = np.digitize(X_pre[:,-1].reshape(-1, 1), bins=bins)\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# encoder.fit(which_bin)\n",
    "X_pre_binned = encoder.transform(which_bin)\n",
    "print(bins)\n",
    "if (np.hstack((train,X_train_binned)).shape[1] == np.hstack((test,X_test_binned)).shape[1]):\n",
    "    if (np.hstack((X_pre,X_pre_binned)).shape[1] == np.hstack((test,X_test_binned)).shape[1]):\n",
    "        train = np.hstack((train,X_train_binned))\n",
    "        test = np.hstack((test,X_test_binned))\n",
    "        X_pre = np.hstack((X_pre,X_pre_binned))\n",
    "        print(\"0k\")\n",
    "else:\n",
    "    print(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119535, 164)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PF = PolynomialFeatures(degree=2)\n",
    "PF.fit(train)\n",
    "X_train = PF.transform(train)\n",
    "X_test = PF.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SS = StandardScaler()\n",
    "SS.fit(train)\n",
    "X_train = SS.transform(train)\n",
    "X_test = SS.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(C = 0.01,penalty='l1')\n",
    "log_reg.fit(train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.091083679259183314"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_train, log_reg.predict_proba(train)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.089388817687746161"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_test, log_reg.predict_proba(test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict_proba(X_pre)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BC = BaggingClassifier(base_estimator=LogisticRegression(C = 0.01), random_state=666, n_estimators=30)\n",
    "# BC.fit(train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log_loss(Y_train, BC.predict_proba(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log_loss(Y_test, BC.predict_proba(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log_reg.predict_proba(train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=500, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "           oob_score=False, random_state=666, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ETR = joblib.load('allETR.pkl')\n",
    "ETR = ExtraTreesClassifier(n_estimators=300, n_jobs=-1, random_state=666, max_depth=500)#0.41\n",
    "ETR.fit(train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.091794363216569669"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_train, ETR.predict_proba(train)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0899327885097429"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_test, ETR.predict_proba(test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joblib.dump(ETR, 'allETR.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB = MultinomialNB(alpha=1)\n",
    "MNB.fit(train+1, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.114825702706991"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_test, MNB.predict_proba(test+1)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BNB = BernoulliNB(alpha=1)\n",
    "BNB.fit(train+1, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.901286592858881"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_test, BNB.predict_proba(test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0008, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last = LogisticRegression(C = 0.0008,penalty='l2')\n",
    "\n",
    "# log_reg.fit(train, Y_train)\n",
    "# ETR.fit(train, Y_train)\n",
    "# MNB.fit(train+1, Y_train)\n",
    "# BNB.fit(train+1, Y_train)\n",
    "c_data = log_reg.predict_proba(train)[:, 1].reshape(-1, 1)\n",
    "c_data = np.hstack((c_data,ETR.predict_proba(train)[:, 1].reshape(-1, 1)))\n",
    "c_data = np.hstack((c_data,MNB.predict_proba(train+1)[:, 1].reshape(-1, 1)))\n",
    "# c_data = np.hstack((c_data,BNB.predict_proba(train+1)[:, 1].reshape(-1, 1)))\n",
    "last.fit(c_data,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pre1 = log_reg.predict_proba(train)[:, 1].reshape(-1, 1)\n",
    "Y_pre2 = ETR.predict_proba(train)[:, 1].reshape(-1, 1)\n",
    "Y_pre3 = MNB.predict_proba(train+1)[:, 1].reshape(-1, 1)\n",
    "# Y_pre4 = BNB.predict_proba(train+1)[:, 1].reshape(-1, 1)\n",
    "Y_add = np.hstack((Y_pre1, Y_pre2))\n",
    "Y_add = np.hstack((Y_add, Y_pre3))\n",
    "# Y_add = np.hstack((Y_add, Y_pre4))\n",
    "Y_pre = last.predict_proba(Y_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042162415180860724"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_train, Y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log_loss(Y_test, last.predict_proba(np.hstack((log_reg.predict_proba(test)[:, 1].reshape(-1, 1),ETR.predict_proba(test)[:, 1].reshape(-1, 1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pre1 = log_reg.predict_proba(test)[:, 1].reshape(-1, 1)\n",
    "Y_pre2 = ETR.predict_proba(test)[:, 1].reshape(-1, 1)\n",
    "Y_pre3 = MNB.predict_proba(test+1)[:, 1].reshape(-1, 1)\n",
    "# Y_pre4 = BNB.predict_proba(test+1)[:, 1].reshape(-1, 1)\n",
    "Y_add = np.hstack((Y_pre1, Y_pre2))\n",
    "Y_add = np.hstack((Y_add, Y_pre3))\n",
    "# Y_add = np.hstack((Y_add, Y_pre4))\n",
    "Y_pre = last.predict_proba(Y_add)\n",
    "\n",
    "# Y_pre = Y_pre1 + Y_pre2 + Y_pre3 + Y_pre4\n",
    "# Y_pre = Y_pre/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.094213842438789233"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_test, Y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99666667,  0.00333333],\n",
       "       [ 0.96666667,  0.03333333],\n",
       "       [ 0.99666667,  0.00333333],\n",
       "       ..., \n",
       "       [ 0.99333333,  0.00666667],\n",
       "       [ 0.99666667,  0.00333333],\n",
       "       [ 0.99      ,  0.01      ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pre = ETR.predict_proba(X_pre)\n",
    "Y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pre = log_reg.predict_proba(X_pre)[:,1]\n",
    "Y_pre = Y_pre.reshape(-1,1)\n",
    "Y_prelist = Y_pre.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sub = pd.DataFrame()\n",
    "# sub['instance_id'] = np.array(data2[['instance_id']],dtype='float').ravel()\n",
    "# sub['predicted_score'] = Y_pre[:,1].ravel()\n",
    "# sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instance_id = np.array(data2[['instance_id']],dtype='float').ravel()\n",
    "# instance_idlist = instance_id.tolist()\n",
    "# with open('123456.txt', 'w') as f:\n",
    "#     f.write(\"instance_id predicted_score\\r\") \n",
    "#     for i in range(Y_pre.shape[0]):\n",
    "#         f.write(str(data2[['instance_id']].iloc[i,0]) + \" \" + str(Y_prelist[i][0]) + '\\r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18371, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data2[['instance_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "data2 = pd.read_table('test.txt',sep=' ',index_col=False) \n",
    "sub['instance_id'] = data2.pop('instance_id')\n",
    "sub['predicted_score'] = Y_pre[:,0].ravel()\n",
    "sub.to_csv('000.txt',index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00775228,  0.02247293,  0.02156624, ...,  0.03197815,\n",
       "        0.03737831,  0.03079344])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
